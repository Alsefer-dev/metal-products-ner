{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "333ff255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from seqeval.metrics import classification_report as seqeval_report\n",
    "from typing import List, Union\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65393be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing all random seeds\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "set_seed(42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ce727e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "def tokenizer(text):\n",
    "    tokens = []    \n",
    "    # RegEx\n",
    "    word_re = re.compile(r'''\n",
    "        \\w+[-\\w]*|      \n",
    "        [^\\w\\s]|         \n",
    "        \\d+\\.\\d+|        \n",
    "        \\d+/\\d+|        \n",
    "        \\d+              \n",
    "    ''', re.VERBOSE)\n",
    "    \n",
    "    for match in word_re.finditer(text):\n",
    "        tokens.append(match.group())\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fbc7703c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "    def __init__(self, data, vocab, tag_encoder):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "        self.tag_encoder = tag_encoder\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data[idx]\n",
    "        tokens = entry[\"tokens\"]\n",
    "        tags = entry[\"tags\"]\n",
    "        \n",
    "        word_ids = [self.vocab.get(token, 1) for token in tokens]\n",
    "        tag_ids = self.tag_encoder.transform(tags)\n",
    "        \n",
    "        return torch.tensor(word_ids, dtype=torch.long), torch.tensor(tag_ids, dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    words, tags = zip(*batch)\n",
    "    words_padded = pad_sequence(words, batch_first=True, padding_value=0)\n",
    "    tags_padded = pad_sequence(tags, batch_first=True, padding_value=-100)\n",
    "    return words_padded, tags_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d21201ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniversalCNN_NER(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size: int,\n",
    "                 num_tags: int,\n",
    "                 embedding_dim: int = 100,\n",
    "                 num_filters: int = 128,\n",
    "                 filter_sizes: Union[int, List[int]] = [3, 5, 7],\n",
    "                 use_lstm: bool = False,\n",
    "                 lstm_hidden: int = 256,\n",
    "                 lstm_num_layers: int = 1,\n",
    "                 bidirectional: bool = True,\n",
    "                 use_layernorm: bool = False,\n",
    "                 use_attention: bool = False,  # Новый параметр\n",
    "                 num_heads: int = 4,         # Количество голов внимания\n",
    "                 attn_dropout: float = 0.1,  # Dropout для внимания\n",
    "                 dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Эмбеддинг\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # CNN слои\n",
    "        if isinstance(filter_sizes, int):\n",
    "            filter_sizes = [filter_sizes]\n",
    "        self.num_conv_layers = len(filter_sizes)\n",
    "        \n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        in_channels = embedding_dim\n",
    "        \n",
    "        for i, fs in enumerate(filter_sizes):\n",
    "            padding = fs // 2\n",
    "            self.conv_layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(\n",
    "                        in_channels=in_channels,\n",
    "                        out_channels=num_filters,\n",
    "                        kernel_size=fs,\n",
    "                        padding=padding,\n",
    "                        padding_mode='zeros'  # Можно изменить на 'reflect' или 'replicate'\n",
    "                    ),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout)\n",
    "                ) \n",
    "            )\n",
    "            in_channels = num_filters\n",
    "        \n",
    "        # LSTM слой\n",
    "        self.use_lstm = use_lstm\n",
    "        if use_lstm:\n",
    "            self.lstm = nn.LSTM(\n",
    "                input_size=num_filters,\n",
    "                hidden_size=lstm_hidden,\n",
    "                num_layers=lstm_num_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                batch_first=True\n",
    "            )\n",
    "            lstm_output_size = lstm_hidden * 2 if bidirectional else lstm_hidden\n",
    "        else:\n",
    "            lstm_output_size = num_filters\n",
    "        \n",
    "        # Механизм внимания\n",
    "        self.use_attention = use_attention\n",
    "        if use_attention:\n",
    "            self.attention = MultiHeadAttention(\n",
    "                embed_dim=lstm_output_size,\n",
    "                num_heads=num_heads,\n",
    "                dropout=attn_dropout\n",
    "            )\n",
    "            self.attn_layer_norm = nn.LayerNorm(lstm_output_size) if use_layernorm else None\n",
    "        \n",
    "        # Нормализация\n",
    "        self.use_layernorm = use_layernorm\n",
    "        if use_layernorm:\n",
    "            self.layernorm = nn.LayerNorm(lstm_output_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(lstm_output_size, num_tags)\n",
    "        \n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # Инициализация CNN\n",
    "        for layer in self.conv_layers:\n",
    "            nn.init.kaiming_normal_(layer[0].weight, mode='fan_out', nonlinearity='relu')\n",
    "            nn.init.constant_(layer[0].bias, 0)\n",
    "        \n",
    "        # Инициализация LSTM\n",
    "        if hasattr(self, 'lstm'):\n",
    "            for name, param in self.lstm.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_normal_(param)\n",
    "                else:\n",
    "                    nn.init.constant_(param, 0)\n",
    "        \n",
    "        # Инициализация выходного слоя\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "        nn.init.constant_(self.fc.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Эмбеддинг\n",
    "        x = self.embedding(x)  \n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # CNN слои\n",
    "        for conv in self.conv_layers:\n",
    "            x = conv(x)\n",
    "        \n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # LSTM слой\n",
    "        if self.use_lstm:\n",
    "            x, _ = self.lstm(x)\n",
    "        \n",
    "        # Механизм внимания\n",
    "        if self.use_attention:\n",
    "            residual = x\n",
    "            x, _ = self.attention(x, x, x)  # Self-attention\n",
    "            x = self.dropout(x)\n",
    "            if self.attn_layer_norm is not None:\n",
    "                x = self.attn_layer_norm(x + residual)\n",
    "        \n",
    "        # Нормализация\n",
    "        if self.use_layernorm and not self.use_attention:  # Для attention уже есть своя нормализация\n",
    "            x = self.layernorm(x)\n",
    "        \n",
    "        return self.fc(self.dropout(x))\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Реализация Multi-Head Self-Attention\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        assert self.head_dim * num_heads == embed_dim, \"Embed dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, key_padding_mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Проецируем и разбиваем на головы\n",
    "        q = self.q_proj(query).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(key).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(value).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Вычисляем attention scores\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        \n",
    "        if key_padding_mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(\n",
    "                key_padding_mask.unsqueeze(1).unsqueeze(2),\n",
    "                float('-inf')\n",
    "            )\n",
    "        \n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Применяем attention weights к values\n",
    "        output = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        # Собираем головы обратно\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_dim)\n",
    "        output = self.out_proj(output)\n",
    "        \n",
    "        return output, attn_weights\n",
    "    \n",
    "# Train\n",
    "def train_model(model, dataloader, num_tags, epochs=20, lr=1e-3, model_save_path='best_model.pt'):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    #criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        model.train()\n",
    "        \n",
    "        for words, tags in dataloader:\n",
    "            words = words.to(device)\n",
    "            tags = tags.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model(words)\n",
    "            \n",
    "            # Правильный reshape\n",
    "            outputs = outputs.reshape(-1, num_tags)\n",
    "            tags = tags.reshape(-1) \n",
    "            \n",
    "            loss = criterion(outputs, tags)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        \n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        \n",
    "        print(f'\\nEpoch {epoch+1}/{epochs} | Avg Loss: {avg_loss:.4f}')\n",
    "    return model\n",
    "\n",
    "# Predicted\n",
    "def predict(model, sentence, vocab, tag_encoder):\n",
    "    model.eval()\n",
    "    word_ids = [vocab.get(word, 1) for word in sentence]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(torch.tensor([word_ids]))\n",
    "        _, predicted = torch.max(outputs, 2)\n",
    "    \n",
    "    predicted_tags = tag_encoder.inverse_transform(predicted.squeeze().numpy())\n",
    "    \n",
    "    entities = []\n",
    "    current_entity = None\n",
    "    start_idx = 0\n",
    "    \n",
    "    for i, (word, tag) in enumerate(zip(sentence, predicted_tags)):\n",
    "        if tag.startswith('B-'):\n",
    "            if current_entity is not None:\n",
    "                entities.append((start_idx, i, current_entity))\n",
    "            current_entity = tag[2:]\n",
    "            start_idx = i\n",
    "        elif tag == 'O' and current_entity is not None:\n",
    "            entities.append((start_idx, i, current_entity))\n",
    "            current_entity = None\n",
    "    \n",
    "    if current_entity is not None:\n",
    "        entities.append((start_idx, len(sentence), current_entity))\n",
    "    \n",
    "    # Return results\n",
    "    for start, end, entity_type in entities:\n",
    "        entity_text = ' '.join(sentence[start:end])\n",
    "        print(f\"{entity_text} -> {entity_type}\")\n",
    "    \n",
    "    return predicted_tags\n",
    "\n",
    "\n",
    "# 6. Предсказание\n",
    "def evaluate_model(model, test_data, vocab, tag_encoder):\n",
    "    model.eval()\n",
    "    all_true_tags = []\n",
    "    all_pred_tags = []\n",
    "    \n",
    "    for entry in test_data:\n",
    "        tokens = entry[\"tokens\"]\n",
    "        true_tags = entry[\"tags\"]\n",
    "        \n",
    "        word_ids = [vocab.get(word, 1) for word in tokens]\n",
    "        with torch.no_grad():\n",
    "            outputs = model(torch.tensor([word_ids]))\n",
    "            _, predicted = torch.max(outputs, 2)\n",
    "        \n",
    "        pred_tags = tag_encoder.inverse_transform(predicted.squeeze().numpy())\n",
    "        \n",
    "        all_true_tags.append(true_tags)\n",
    "        all_pred_tags.append(pred_tags.tolist())\n",
    "    \n",
    "    # Entity-level\n",
    "    entity_report = seqeval_report(all_true_tags, all_pred_tags, zero_division=0)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    entity_types = set(tag[2:] for tags in all_true_tags for tag in tags if tag != 'O')\n",
    "    entity_metrics = {}\n",
    "    \n",
    "    for entity in entity_types:\n",
    "        tp = 0  # True Positives\n",
    "        fp = 0  # False Positives\n",
    "        fn = 0  # False Negatives\n",
    "        \n",
    "        for true_seq, pred_seq in zip(all_true_tags, all_pred_tags):\n",
    "            true_entities = get_entities(true_seq)\n",
    "            pred_entities = get_entities(pred_seq)\n",
    "            \n",
    "            true_set = set((start, end) for (start, end, e_type) in true_entities if e_type == entity)\n",
    "            pred_set = set((start, end) for (start, end, e_type) in pred_entities if e_type == entity)\n",
    "            \n",
    "            tp += len(true_set & pred_set)\n",
    "            fp += len(pred_set - true_set)\n",
    "            fn += len(true_set - pred_set)\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        entity_metrics[entity] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'support': tp + fn\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'entity_level': entity_report,\n",
    "        'entity_metrics': entity_metrics\n",
    "    }\n",
    "\n",
    "def get_entities(tag_sequence):\n",
    "    entities = []\n",
    "    current_entity = None\n",
    "    start_idx = 0\n",
    "    \n",
    "    for i, tag in enumerate(tag_sequence):\n",
    "        if tag.startswith('B-'):\n",
    "            if current_entity is not None:\n",
    "                entities.append((start_idx, i, current_entity))\n",
    "            current_entity = tag[2:]\n",
    "            start_idx = i\n",
    "        elif tag == 'O' and current_entity is not None:\n",
    "            entities.append((start_idx, i, current_entity))\n",
    "            current_entity = None\n",
    "    \n",
    "    if current_entity is not None:\n",
    "        entities.append((start_idx, len(tag_sequence), current_entity))\n",
    "    \n",
    "    return entities    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "faa5b1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2051\n",
      "879\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "with open(r\"..\\..\\data\\raw\\dataset_train.json\", 'r', encoding='utf-8') as fp:\n",
    "    #print(fp.read(4544900))\n",
    "    sample_data_train = json.load(fp)\n",
    "print(len(sample_data_train))\n",
    "\n",
    "with open(r\"..\\..\\data\\raw\\dataset_test.json\", 'r', encoding='utf-8') as fp:\n",
    "    sample_data_test = json.load(fp)\n",
    "print(len(sample_data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d2e8193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary\n",
    "word_counts = defaultdict(int)\n",
    "all_tags = []\n",
    "\n",
    "for entry in sample_data_train:\n",
    "    for token in entry[\"tokens\"]:\n",
    "        word_counts[token] += 1\n",
    "    all_tags.extend(entry[\"tags\"])\n",
    "\n",
    "vocab = {word: i+2 for i, word in enumerate(word_counts)}  # 0 для padding, 1 для UNK\n",
    "vocab_size = len(vocab) + 2\n",
    "\n",
    "tag_encoder = LabelEncoder()\n",
    "tag_encoder.fit(all_tags)\n",
    "num_tags = len(tag_encoder.classes_)\n",
    "\n",
    "dataset = NERDataset(sample_data_train, vocab, tag_encoder)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec8a4d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.1176\n",
      "\n",
      "Epoch 1/20 | Avg Loss: 0.6190\n",
      "Epoch 2, Loss: 0.0191\n",
      "\n",
      "Epoch 2/20 | Avg Loss: 0.1794\n",
      "Epoch 3, Loss: 0.1989\n",
      "\n",
      "Epoch 3/20 | Avg Loss: 0.1082\n",
      "Epoch 4, Loss: 0.0254\n",
      "\n",
      "Epoch 4/20 | Avg Loss: 0.0806\n",
      "Epoch 5, Loss: 0.0136\n",
      "\n",
      "Epoch 5/20 | Avg Loss: 0.0674\n",
      "Epoch 6, Loss: 0.0375\n",
      "\n",
      "Epoch 6/20 | Avg Loss: 0.0532\n",
      "Epoch 7, Loss: 0.0240\n",
      "\n",
      "Epoch 7/20 | Avg Loss: 0.0442\n",
      "Epoch 8, Loss: 0.0746\n",
      "\n",
      "Epoch 8/20 | Avg Loss: 0.0391\n",
      "Epoch 9, Loss: 0.0503\n",
      "\n",
      "Epoch 9/20 | Avg Loss: 0.0321\n",
      "Epoch 10, Loss: 0.0544\n",
      "\n",
      "Epoch 10/20 | Avg Loss: 0.0407\n",
      "Epoch 11, Loss: 0.0003\n",
      "\n",
      "Epoch 11/20 | Avg Loss: 0.0508\n",
      "Epoch 12, Loss: 0.0005\n",
      "\n",
      "Epoch 12/20 | Avg Loss: 0.0269\n",
      "Epoch 13, Loss: 0.0045\n",
      "\n",
      "Epoch 13/20 | Avg Loss: 0.0217\n",
      "Epoch 14, Loss: 0.0210\n",
      "\n",
      "Epoch 14/20 | Avg Loss: 0.0249\n",
      "Epoch 15, Loss: 0.0338\n",
      "\n",
      "Epoch 15/20 | Avg Loss: 0.0425\n",
      "Epoch 16, Loss: 0.0172\n",
      "\n",
      "Epoch 16/20 | Avg Loss: 0.0234\n",
      "Epoch 17, Loss: 0.0008\n",
      "\n",
      "Epoch 17/20 | Avg Loss: 0.0184\n",
      "Epoch 18, Loss: 0.0001\n",
      "\n",
      "Epoch 18/20 | Avg Loss: 0.0158\n",
      "Epoch 19, Loss: 0.0171\n",
      "\n",
      "Epoch 19/20 | Avg Loss: 0.0207\n",
      "Epoch 20, Loss: 0.0175\n",
      "\n",
      "Epoch 20/20 | Avg Loss: 0.0495\n",
      "Time train:  588.4127397537231 c\n",
      "\n",
      "=== Entity-level Metrics ===\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           coating       0.99      0.91      0.95       203\n",
      "             color       0.94      0.86      0.90       107\n",
      "           country       0.81      0.84      0.82        25\n",
      "              form       0.95      0.97      0.96       231\n",
      "            height       0.99      0.96      0.97       581\n",
      "        height_big       0.98      0.97      0.98       198\n",
      "      height_small       1.00      0.73      0.84        11\n",
      "    inner_diameter       0.99      0.97      0.98      1616\n",
      "            length       0.98      0.95      0.96      1714\n",
      "      manufacturer       0.99      0.84      0.91       153\n",
      "              mark       0.97      0.97      0.97      1043\n",
      "        mark_steal       0.99      0.99      0.99     10763\n",
      "   mark_steel_aisi       0.99      1.00      1.00      1333\n",
      "          material       0.99      0.97      0.98      2073\n",
      "    outer_diameter       0.99      0.94      0.97       604\n",
      "           package       0.74      0.93      0.82        30\n",
      "         precision       0.96      1.00      0.98        89\n",
      "           product       0.99      0.99      0.99      4783\n",
      "           purpose       0.97      0.70      0.81        50\n",
      "       standart_en       1.00      1.00      1.00       915\n",
      "     standart_gost       1.00      1.00      1.00      5271\n",
      "       standart_tu       1.00      0.95      0.97       364\n",
      "    strength_class       0.99      0.98      0.98       305\n",
      "strength_class_old       0.94      0.96      0.95       190\n",
      "         tehnology       0.99      0.99      0.99      1789\n",
      "         thickness       0.99      0.99      0.99      4632\n",
      "              type       1.00      0.98      0.99      1932\n",
      "             width       0.97      0.98      0.97      2020\n",
      "\n",
      "         micro avg       0.99      0.98      0.99     43025\n",
      "         macro avg       0.97      0.94      0.95     43025\n",
      "      weighted avg       0.99      0.98      0.99     43025\n",
      "\n",
      "\n",
      "=== Per-entity Metrics ===\n",
      "outer_diameter:\n",
      "  Precision: 0.9913\n",
      "  Recall:    0.9437\n",
      "  F1-score:  0.9669\n",
      "  Support:   604\n",
      "strength_class:\n",
      "  Precision: 0.9900\n",
      "  Recall:    0.9770\n",
      "  F1-score:  0.9835\n",
      "  Support:   305\n",
      "length:\n",
      "  Precision: 0.9777\n",
      "  Recall:    0.9463\n",
      "  F1-score:  0.9618\n",
      "  Support:   1714\n",
      "standart_gost:\n",
      "  Precision: 0.9964\n",
      "  Recall:    0.9953\n",
      "  F1-score:  0.9958\n",
      "  Support:   5271\n",
      "coating:\n",
      "  Precision: 0.9946\n",
      "  Recall:    0.9064\n",
      "  F1-score:  0.9485\n",
      "  Support:   203\n",
      "inner_diameter:\n",
      "  Precision: 0.9930\n",
      "  Recall:    0.9697\n",
      "  F1-score:  0.9812\n",
      "  Support:   1616\n",
      "height:\n",
      "  Precision: 0.9876\n",
      "  Recall:    0.9587\n",
      "  F1-score:  0.9729\n",
      "  Support:   581\n",
      "standart_en:\n",
      "  Precision: 0.9956\n",
      "  Recall:    0.9956\n",
      "  F1-score:  0.9956\n",
      "  Support:   915\n",
      "color:\n",
      "  Precision: 0.9388\n",
      "  Recall:    0.8598\n",
      "  F1-score:  0.8976\n",
      "  Support:   107\n",
      "manufacturer:\n",
      "  Precision: 0.9922\n",
      "  Recall:    0.8366\n",
      "  F1-score:  0.9078\n",
      "  Support:   153\n",
      "standart_tu:\n",
      "  Precision: 1.0000\n",
      "  Recall:    0.9505\n",
      "  F1-score:  0.9746\n",
      "  Support:   364\n",
      "height_big:\n",
      "  Precision: 0.9847\n",
      "  Recall:    0.9747\n",
      "  F1-score:  0.9797\n",
      "  Support:   198\n",
      "mark_steel_aisi:\n",
      "  Precision: 0.9933\n",
      "  Recall:    0.9985\n",
      "  F1-score:  0.9959\n",
      "  Support:   1333\n",
      "product:\n",
      "  Precision: 0.9943\n",
      "  Recall:    0.9921\n",
      "  F1-score:  0.9932\n",
      "  Support:   4783\n",
      "height_small:\n",
      "  Precision: 1.0000\n",
      "  Recall:    0.7273\n",
      "  F1-score:  0.8421\n",
      "  Support:   11\n",
      "form:\n",
      "  Precision: 0.9494\n",
      "  Recall:    0.9740\n",
      "  F1-score:  0.9615\n",
      "  Support:   231\n",
      "width:\n",
      "  Precision: 0.9664\n",
      "  Recall:    0.9832\n",
      "  F1-score:  0.9747\n",
      "  Support:   2020\n",
      "strength_class_old:\n",
      "  Precision: 0.9433\n",
      "  Recall:    0.9632\n",
      "  F1-score:  0.9531\n",
      "  Support:   190\n",
      "mark:\n",
      "  Precision: 0.9693\n",
      "  Recall:    0.9703\n",
      "  F1-score:  0.9698\n",
      "  Support:   1043\n",
      "country:\n",
      "  Precision: 0.8077\n",
      "  Recall:    0.8400\n",
      "  F1-score:  0.8235\n",
      "  Support:   25\n",
      "purpose:\n",
      "  Precision: 0.9722\n",
      "  Recall:    0.7000\n",
      "  F1-score:  0.8140\n",
      "  Support:   50\n",
      "type:\n",
      "  Precision: 0.9969\n",
      "  Recall:    0.9834\n",
      "  F1-score:  0.9901\n",
      "  Support:   1932\n",
      "thickness:\n",
      "  Precision: 0.9871\n",
      "  Recall:    0.9903\n",
      "  F1-score:  0.9887\n",
      "  Support:   4632\n",
      "package:\n",
      "  Precision: 0.7368\n",
      "  Recall:    0.9333\n",
      "  F1-score:  0.8235\n",
      "  Support:   30\n",
      "mark_steal:\n",
      "  Precision: 0.9943\n",
      "  Recall:    0.9928\n",
      "  F1-score:  0.9935\n",
      "  Support:   10763\n",
      "precision:\n",
      "  Precision: 0.9570\n",
      "  Recall:    1.0000\n",
      "  F1-score:  0.9780\n",
      "  Support:   89\n",
      "material:\n",
      "  Precision: 0.9926\n",
      "  Recall:    0.9749\n",
      "  F1-score:  0.9837\n",
      "  Support:   2073\n",
      "tehnology:\n",
      "  Precision: 0.9916\n",
      "  Recall:    0.9883\n",
      "  F1-score:  0.9899\n",
      "  Support:   1789\n",
      "Time inference:  4.59110689163208  c\n"
     ]
    }
   ],
   "source": [
    "# Train simple CNN (1 layer) + BiLSTM + Layernorm + Att\n",
    "time_start = time.time()\n",
    "model = UniversalCNN_NER(\n",
    "    vocab_size=10000,\n",
    "    embedding_dim=128,\n",
    "    num_tags=len(tag_encoder.classes_),\n",
    "    filter_sizes=[3],\n",
    "    num_filters=128,\n",
    "    use_lstm=True,\n",
    "    use_attention=True,\n",
    "    num_heads=4,\n",
    "    use_layernorm=True\n",
    ")\n",
    "\n",
    "trained_model = train_model(\n",
    "    model=model,\n",
    "    dataloader=dataloader,\n",
    "    num_tags=len(tag_encoder.classes_),\n",
    "    epochs=20,\n",
    "    lr=1e-3\n",
    ")\n",
    "\n",
    "print('Time train: ', time.time() - time_start, 'c')\n",
    "\n",
    "time_start = time.time()\n",
    "metrics = evaluate_model(model, sample_data_test, vocab, tag_encoder)\n",
    "\n",
    "print(\"\\n=== Entity-level Metrics ===\")\n",
    "print(metrics['entity_level'])\n",
    "\n",
    "print(\"\\n=== Per-entity Metrics ===\")\n",
    "for entity, scores in metrics['entity_metrics'].items():\n",
    "    print(f\"{entity}:\")\n",
    "    print(f\"  Precision: {scores['precision']:.4f}\")\n",
    "    print(f\"  Recall:    {scores['recall']:.4f}\")\n",
    "    print(f\"  F1-score:  {scores['f1']:.4f}\")\n",
    "    print(f\"  Support:   {scores['support']}\")\n",
    "print(\"Time inference: \", time.time() - time_start, ' c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2abd1445-db4a-418d-851f-321b08e5659f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.1315\n",
      "\n",
      "Epoch 1/20 | Avg Loss: 0.4104\n",
      "Epoch 2, Loss: 0.0461\n",
      "\n",
      "Epoch 2/20 | Avg Loss: 0.0986\n",
      "Epoch 3, Loss: 0.1434\n",
      "\n",
      "Epoch 3/20 | Avg Loss: 0.0600\n",
      "Epoch 4, Loss: 0.0125\n",
      "\n",
      "Epoch 4/20 | Avg Loss: 0.0384\n",
      "Epoch 5, Loss: 0.0204\n",
      "\n",
      "Epoch 5/20 | Avg Loss: 0.0364\n",
      "Epoch 6, Loss: 0.1405\n",
      "\n",
      "Epoch 6/20 | Avg Loss: 0.1087\n",
      "Epoch 7, Loss: 0.0017\n",
      "\n",
      "Epoch 7/20 | Avg Loss: 0.0340\n",
      "Epoch 8, Loss: 0.0001\n",
      "\n",
      "Epoch 8/20 | Avg Loss: 0.0100\n",
      "Epoch 9, Loss: 0.0009\n",
      "\n",
      "Epoch 9/20 | Avg Loss: 0.0069\n",
      "Epoch 10, Loss: 0.0208\n",
      "\n",
      "Epoch 10/20 | Avg Loss: 0.0269\n",
      "Epoch 11, Loss: 0.0192\n",
      "\n",
      "Epoch 11/20 | Avg Loss: 0.0202\n",
      "Epoch 12, Loss: 0.0003\n",
      "\n",
      "Epoch 12/20 | Avg Loss: 0.0138\n",
      "Epoch 13, Loss: 0.0274\n",
      "\n",
      "Epoch 13/20 | Avg Loss: 0.0718\n",
      "Epoch 14, Loss: 0.0015\n",
      "\n",
      "Epoch 14/20 | Avg Loss: 0.0218\n",
      "Epoch 15, Loss: 0.0002\n",
      "\n",
      "Epoch 15/20 | Avg Loss: 0.0076\n",
      "Epoch 16, Loss: 0.0069\n",
      "\n",
      "Epoch 16/20 | Avg Loss: 0.0055\n",
      "Epoch 17, Loss: 0.0509\n",
      "\n",
      "Epoch 17/20 | Avg Loss: 0.0346\n",
      "Epoch 18, Loss: 0.0816\n",
      "\n",
      "Epoch 18/20 | Avg Loss: 0.0223\n",
      "Epoch 19, Loss: 0.0001\n",
      "\n",
      "Epoch 19/20 | Avg Loss: 0.0078\n",
      "Epoch 20, Loss: 0.0108\n",
      "\n",
      "Epoch 20/20 | Avg Loss: 0.0049\n",
      "Time train:  727.7123913764954 c\n",
      "\n",
      "=== Entity-level Metrics ===\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           coating       1.00      0.90      0.95       203\n",
      "             color       0.93      0.95      0.94       107\n",
      "           country       1.00      0.80      0.89        25\n",
      "              form       0.95      0.97      0.96       231\n",
      "            height       0.98      0.96      0.97       581\n",
      "        height_big       0.98      0.98      0.98       198\n",
      "      height_small       0.89      0.73      0.80        11\n",
      "    inner_diameter       0.99      0.99      0.99      1616\n",
      "            length       0.97      0.97      0.97      1714\n",
      "      manufacturer       0.96      0.87      0.91       153\n",
      "              mark       0.96      0.98      0.97      1043\n",
      "        mark_steal       0.99      0.99      0.99     10763\n",
      "   mark_steel_aisi       0.99      0.99      0.99      1333\n",
      "          material       0.99      0.98      0.98      2073\n",
      "    outer_diameter       0.99      0.96      0.98       604\n",
      "           package       0.83      0.97      0.89        30\n",
      "         precision       1.00      1.00      1.00        89\n",
      "           product       1.00      0.99      0.99      4783\n",
      "           purpose       0.93      0.78      0.85        50\n",
      "       standart_en       1.00      0.98      0.99       915\n",
      "     standart_gost       0.99      1.00      0.99      5271\n",
      "       standart_tu       0.99      0.97      0.98       364\n",
      "    strength_class       1.00      1.00      1.00       305\n",
      "strength_class_old       0.97      1.00      0.99       190\n",
      "         tehnology       0.99      0.98      0.99      1789\n",
      "         thickness       0.99      0.98      0.99      4632\n",
      "              type       0.98      0.99      0.98      1932\n",
      "             width       0.97      0.98      0.97      2020\n",
      "\n",
      "         micro avg       0.99      0.99      0.99     43025\n",
      "         macro avg       0.97      0.95      0.96     43025\n",
      "      weighted avg       0.99      0.99      0.99     43025\n",
      "\n",
      "\n",
      "=== Per-entity Metrics ===\n",
      "outer_diameter:\n",
      "  Precision: 0.9898\n",
      "  Recall:    0.9636\n",
      "  F1-score:  0.9765\n",
      "  Support:   604\n",
      "strength_class:\n",
      "  Precision: 0.9967\n",
      "  Recall:    0.9967\n",
      "  F1-score:  0.9967\n",
      "  Support:   305\n",
      "length:\n",
      "  Precision: 0.9657\n",
      "  Recall:    0.9691\n",
      "  F1-score:  0.9674\n",
      "  Support:   1714\n",
      "standart_gost:\n",
      "  Precision: 0.9878\n",
      "  Recall:    0.9968\n",
      "  F1-score:  0.9923\n",
      "  Support:   5271\n",
      "coating:\n",
      "  Precision: 1.0000\n",
      "  Recall:    0.8966\n",
      "  F1-score:  0.9455\n",
      "  Support:   203\n",
      "inner_diameter:\n",
      "  Precision: 0.9864\n",
      "  Recall:    0.9882\n",
      "  F1-score:  0.9873\n",
      "  Support:   1616\n",
      "height:\n",
      "  Precision: 0.9788\n",
      "  Recall:    0.9552\n",
      "  F1-score:  0.9669\n",
      "  Support:   581\n",
      "standart_en:\n",
      "  Precision: 0.9967\n",
      "  Recall:    0.9792\n",
      "  F1-score:  0.9879\n",
      "  Support:   915\n",
      "color:\n",
      "  Precision: 0.9273\n",
      "  Recall:    0.9533\n",
      "  F1-score:  0.9401\n",
      "  Support:   107\n",
      "manufacturer:\n",
      "  Precision: 0.9568\n",
      "  Recall:    0.8693\n",
      "  F1-score:  0.9110\n",
      "  Support:   153\n",
      "standart_tu:\n",
      "  Precision: 0.9916\n",
      "  Recall:    0.9698\n",
      "  F1-score:  0.9806\n",
      "  Support:   364\n",
      "height_big:\n",
      "  Precision: 0.9848\n",
      "  Recall:    0.9798\n",
      "  F1-score:  0.9823\n",
      "  Support:   198\n",
      "mark_steel_aisi:\n",
      "  Precision: 0.9902\n",
      "  Recall:    0.9895\n",
      "  F1-score:  0.9899\n",
      "  Support:   1333\n",
      "product:\n",
      "  Precision: 0.9956\n",
      "  Recall:    0.9904\n",
      "  F1-score:  0.9930\n",
      "  Support:   4783\n",
      "height_small:\n",
      "  Precision: 0.8889\n",
      "  Recall:    0.7273\n",
      "  F1-score:  0.8000\n",
      "  Support:   11\n",
      "form:\n",
      "  Precision: 0.9489\n",
      "  Recall:    0.9654\n",
      "  F1-score:  0.9571\n",
      "  Support:   231\n",
      "width:\n",
      "  Precision: 0.9710\n",
      "  Recall:    0.9772\n",
      "  F1-score:  0.9741\n",
      "  Support:   2020\n",
      "strength_class_old:\n",
      "  Precision: 0.9744\n",
      "  Recall:    1.0000\n",
      "  F1-score:  0.9870\n",
      "  Support:   190\n",
      "mark:\n",
      "  Precision: 0.9604\n",
      "  Recall:    0.9770\n",
      "  F1-score:  0.9686\n",
      "  Support:   1043\n",
      "country:\n",
      "  Precision: 1.0000\n",
      "  Recall:    0.8000\n",
      "  F1-score:  0.8889\n",
      "  Support:   25\n",
      "purpose:\n",
      "  Precision: 0.9286\n",
      "  Recall:    0.7800\n",
      "  F1-score:  0.8478\n",
      "  Support:   50\n",
      "type:\n",
      "  Precision: 0.9765\n",
      "  Recall:    0.9891\n",
      "  F1-score:  0.9828\n",
      "  Support:   1932\n",
      "thickness:\n",
      "  Precision: 0.9896\n",
      "  Recall:    0.9849\n",
      "  F1-score:  0.9872\n",
      "  Support:   4632\n",
      "package:\n",
      "  Precision: 0.8286\n",
      "  Recall:    0.9667\n",
      "  F1-score:  0.8923\n",
      "  Support:   30\n",
      "mark_steal:\n",
      "  Precision: 0.9878\n",
      "  Recall:    0.9927\n",
      "  F1-score:  0.9902\n",
      "  Support:   10763\n",
      "precision:\n",
      "  Precision: 1.0000\n",
      "  Recall:    1.0000\n",
      "  F1-score:  1.0000\n",
      "  Support:   89\n",
      "material:\n",
      "  Precision: 0.9874\n",
      "  Recall:    0.9807\n",
      "  F1-score:  0.9840\n",
      "  Support:   2073\n",
      "tehnology:\n",
      "  Precision: 0.9938\n",
      "  Recall:    0.9782\n",
      "  F1-score:  0.9859\n",
      "  Support:   1789\n",
      "Time inference:  3.629586696624756  c\n"
     ]
    }
   ],
   "source": [
    "# Train simple BiLSTM + Layernorm + Att\n",
    "time_start = time.time()\n",
    "model = UniversalCNN_NER(\n",
    "    vocab_size=10000,\n",
    "    embedding_dim=128,\n",
    "    num_tags=len(tag_encoder.classes_),\n",
    "    filter_sizes=[],\n",
    "    num_filters=128,\n",
    "    use_lstm=True,\n",
    "    use_attention=True,\n",
    "    num_heads=4,\n",
    "    use_layernorm=True\n",
    ")\n",
    "\n",
    "trained_model = train_model(\n",
    "    model=model,\n",
    "    dataloader=dataloader,\n",
    "    num_tags=len(tag_encoder.classes_),\n",
    "    epochs=20,\n",
    "    lr=1e-3\n",
    ")\n",
    "\n",
    "print('Time train: ', time.time() - time_start, 'c')\n",
    "\n",
    "time_start = time.time()\n",
    "metrics = evaluate_model(model, sample_data_test, vocab, tag_encoder)\n",
    "\n",
    "print(\"\\n=== Entity-level Metrics ===\")\n",
    "print(metrics['entity_level'])\n",
    "\n",
    "print(\"\\n=== Per-entity Metrics ===\")\n",
    "for entity, scores in metrics['entity_metrics'].items():\n",
    "    print(f\"{entity}:\")\n",
    "    print(f\"  Precision: {scores['precision']:.4f}\")\n",
    "    print(f\"  Recall:    {scores['recall']:.4f}\")\n",
    "    print(f\"  F1-score:  {scores['f1']:.4f}\")\n",
    "    print(f\"  Support:   {scores['support']}\")\n",
    "print(\"Time inference: \", time.time() - time_start, ' c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "989aa54e-9144-4906-8511-5c019082f8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.1251\n",
      "\n",
      "Epoch 1/20 | Avg Loss: 0.4756\n",
      "Epoch 2, Loss: 0.0028\n",
      "\n",
      "Epoch 2/20 | Avg Loss: 0.1089\n",
      "Epoch 3, Loss: 0.0008\n",
      "\n",
      "Epoch 3/20 | Avg Loss: 0.0572\n",
      "Epoch 4, Loss: 0.0419\n",
      "\n",
      "Epoch 4/20 | Avg Loss: 0.0334\n",
      "Epoch 5, Loss: 0.0102\n",
      "\n",
      "Epoch 5/20 | Avg Loss: 0.0203\n",
      "Epoch 6, Loss: 0.0141\n",
      "\n",
      "Epoch 6/20 | Avg Loss: 0.0168\n",
      "Epoch 7, Loss: 0.0431\n",
      "\n",
      "Epoch 7/20 | Avg Loss: 0.1329\n",
      "Epoch 8, Loss: 0.0006\n",
      "\n",
      "Epoch 8/20 | Avg Loss: 0.0246\n",
      "Epoch 9, Loss: 0.0008\n",
      "\n",
      "Epoch 9/20 | Avg Loss: 0.0081\n",
      "Epoch 10, Loss: 0.0446\n",
      "\n",
      "Epoch 10/20 | Avg Loss: 0.0050\n",
      "Epoch 11, Loss: 0.0006\n",
      "\n",
      "Epoch 11/20 | Avg Loss: 0.0101\n",
      "Epoch 12, Loss: 0.0013\n",
      "\n",
      "Epoch 12/20 | Avg Loss: 0.0144\n",
      "Epoch 13, Loss: 0.0205\n",
      "\n",
      "Epoch 13/20 | Avg Loss: 0.0363\n",
      "Epoch 14, Loss: 0.0004\n",
      "\n",
      "Epoch 14/20 | Avg Loss: 0.0180\n",
      "Epoch 15, Loss: 0.0422\n",
      "\n",
      "Epoch 15/20 | Avg Loss: 0.0104\n",
      "Epoch 16, Loss: 0.0003\n",
      "\n",
      "Epoch 16/20 | Avg Loss: 0.0041\n",
      "Epoch 17, Loss: 0.0242\n",
      "\n",
      "Epoch 17/20 | Avg Loss: 0.0043\n",
      "Epoch 18, Loss: 0.0008\n",
      "\n",
      "Epoch 18/20 | Avg Loss: 0.0124\n",
      "Epoch 19, Loss: 0.0004\n",
      "\n",
      "Epoch 19/20 | Avg Loss: 0.0151\n",
      "Epoch 20, Loss: 0.0141\n",
      "\n",
      "Epoch 20/20 | Avg Loss: 0.0055\n",
      "Time train:  1005.6084055900574 c\n",
      "\n",
      "=== Entity-level Metrics ===\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           coating       0.98      0.91      0.94       203\n",
      "             color       1.00      0.96      0.98       107\n",
      "           country       1.00      0.84      0.91        25\n",
      "              form       1.00      0.97      0.98       231\n",
      "            height       0.98      0.97      0.97       581\n",
      "        height_big       0.99      0.98      0.99       198\n",
      "      height_small       0.89      0.73      0.80        11\n",
      "    inner_diameter       0.99      0.98      0.98      1616\n",
      "            length       0.98      0.95      0.96      1714\n",
      "      manufacturer       1.00      0.87      0.93       153\n",
      "              mark       0.96      0.97      0.97      1043\n",
      "        mark_steal       0.99      0.99      0.99     10763\n",
      "   mark_steel_aisi       0.99      1.00      1.00      1333\n",
      "          material       0.99      0.98      0.99      2073\n",
      "    outer_diameter       0.98      0.96      0.97       604\n",
      "           package       0.88      1.00      0.94        30\n",
      "         precision       1.00      1.00      1.00        89\n",
      "           product       0.98      0.99      0.99      4783\n",
      "           purpose       0.93      0.78      0.85        50\n",
      "       standart_en       0.99      1.00      1.00       915\n",
      "     standart_gost       1.00      0.99      1.00      5271\n",
      "       standart_tu       1.00      0.91      0.95       364\n",
      "    strength_class       0.98      1.00      0.99       305\n",
      "strength_class_old       0.99      1.00      0.99       190\n",
      "         tehnology       0.98      0.99      0.98      1789\n",
      "         thickness       0.99      0.99      0.99      4632\n",
      "              type       1.00      0.99      0.99      1932\n",
      "             width       0.98      0.96      0.97      2020\n",
      "\n",
      "         micro avg       0.99      0.99      0.99     43025\n",
      "         macro avg       0.98      0.95      0.96     43025\n",
      "      weighted avg       0.99      0.99      0.99     43025\n",
      "\n",
      "\n",
      "=== Per-entity Metrics ===\n",
      "outer_diameter:\n",
      "  Precision: 0.9765\n",
      "  Recall:    0.9619\n",
      "  F1-score:  0.9691\n",
      "  Support:   604\n",
      "strength_class:\n",
      "  Precision: 0.9838\n",
      "  Recall:    0.9967\n",
      "  F1-score:  0.9902\n",
      "  Support:   305\n",
      "length:\n",
      "  Precision: 0.9772\n",
      "  Recall:    0.9498\n",
      "  F1-score:  0.9633\n",
      "  Support:   1714\n",
      "standart_gost:\n",
      "  Precision: 0.9958\n",
      "  Recall:    0.9947\n",
      "  F1-score:  0.9953\n",
      "  Support:   5271\n",
      "coating:\n",
      "  Precision: 0.9787\n",
      "  Recall:    0.9064\n",
      "  F1-score:  0.9412\n",
      "  Support:   203\n",
      "inner_diameter:\n",
      "  Precision: 0.9887\n",
      "  Recall:    0.9783\n",
      "  F1-score:  0.9835\n",
      "  Support:   1616\n",
      "height:\n",
      "  Precision: 0.9757\n",
      "  Recall:    0.9656\n",
      "  F1-score:  0.9706\n",
      "  Support:   581\n",
      "standart_en:\n",
      "  Precision: 0.9946\n",
      "  Recall:    0.9989\n",
      "  F1-score:  0.9967\n",
      "  Support:   915\n",
      "color:\n",
      "  Precision: 1.0000\n",
      "  Recall:    0.9626\n",
      "  F1-score:  0.9810\n",
      "  Support:   107\n",
      "manufacturer:\n",
      "  Precision: 1.0000\n",
      "  Recall:    0.8693\n",
      "  F1-score:  0.9301\n",
      "  Support:   153\n",
      "standart_tu:\n",
      "  Precision: 1.0000\n",
      "  Recall:    0.9066\n",
      "  F1-score:  0.9510\n",
      "  Support:   364\n",
      "height_big:\n",
      "  Precision: 0.9949\n",
      "  Recall:    0.9798\n",
      "  F1-score:  0.9873\n",
      "  Support:   198\n",
      "mark_steel_aisi:\n",
      "  Precision: 0.9948\n",
      "  Recall:    0.9962\n",
      "  F1-score:  0.9955\n",
      "  Support:   1333\n",
      "product:\n",
      "  Precision: 0.9834\n",
      "  Recall:    0.9923\n",
      "  F1-score:  0.9878\n",
      "  Support:   4783\n",
      "height_small:\n",
      "  Precision: 0.8889\n",
      "  Recall:    0.7273\n",
      "  F1-score:  0.8000\n",
      "  Support:   11\n",
      "form:\n",
      "  Precision: 1.0000\n",
      "  Recall:    0.9654\n",
      "  F1-score:  0.9824\n",
      "  Support:   231\n",
      "width:\n",
      "  Precision: 0.9828\n",
      "  Recall:    0.9619\n",
      "  F1-score:  0.9722\n",
      "  Support:   2020\n",
      "strength_class_old:\n",
      "  Precision: 0.9896\n",
      "  Recall:    1.0000\n",
      "  F1-score:  0.9948\n",
      "  Support:   190\n",
      "mark:\n",
      "  Precision: 0.9611\n",
      "  Recall:    0.9703\n",
      "  F1-score:  0.9656\n",
      "  Support:   1043\n",
      "country:\n",
      "  Precision: 1.0000\n",
      "  Recall:    0.8400\n",
      "  F1-score:  0.9130\n",
      "  Support:   25\n",
      "purpose:\n",
      "  Precision: 0.9286\n",
      "  Recall:    0.7800\n",
      "  F1-score:  0.8478\n",
      "  Support:   50\n",
      "type:\n",
      "  Precision: 0.9963\n",
      "  Recall:    0.9865\n",
      "  F1-score:  0.9914\n",
      "  Support:   1932\n",
      "thickness:\n",
      "  Precision: 0.9873\n",
      "  Recall:    0.9888\n",
      "  F1-score:  0.9880\n",
      "  Support:   4632\n",
      "package:\n",
      "  Precision: 0.8824\n",
      "  Recall:    1.0000\n",
      "  F1-score:  0.9375\n",
      "  Support:   30\n",
      "mark_steal:\n",
      "  Precision: 0.9855\n",
      "  Recall:    0.9949\n",
      "  F1-score:  0.9902\n",
      "  Support:   10763\n",
      "precision:\n",
      "  Precision: 1.0000\n",
      "  Recall:    1.0000\n",
      "  F1-score:  1.0000\n",
      "  Support:   89\n",
      "material:\n",
      "  Precision: 0.9908\n",
      "  Recall:    0.9846\n",
      "  F1-score:  0.9877\n",
      "  Support:   2073\n",
      "tehnology:\n",
      "  Precision: 0.9811\n",
      "  Recall:    0.9883\n",
      "  F1-score:  0.9847\n",
      "  Support:   1789\n",
      "Time inference:  5.967979907989502  c\n"
     ]
    }
   ],
   "source": [
    "# Train simple BiLSTM 2 + Layernorm + Att\n",
    "time_start = time.time()\n",
    "model = UniversalCNN_NER(\n",
    "    vocab_size=10000,\n",
    "    embedding_dim=128,\n",
    "    num_tags=len(tag_encoder.classes_),\n",
    "    filter_sizes=[],\n",
    "    num_filters=128,\n",
    "    use_lstm=True,\n",
    "    lstm_num_layers=2,\n",
    "    use_attention=True,\n",
    "    num_heads=4,\n",
    "    use_layernorm=True\n",
    ")\n",
    "\n",
    "trained_model = train_model(\n",
    "    model=model,\n",
    "    dataloader=dataloader,\n",
    "    num_tags=len(tag_encoder.classes_),\n",
    "    epochs=20,\n",
    "    lr=1e-3\n",
    ")\n",
    "\n",
    "print('Time train: ', time.time() - time_start, 'c')\n",
    "\n",
    "time_start = time.time()\n",
    "metrics = evaluate_model(model, sample_data_test, vocab, tag_encoder)\n",
    "\n",
    "print(\"\\n=== Entity-level Metrics ===\")\n",
    "print(metrics['entity_level'])\n",
    "\n",
    "print(\"\\n=== Per-entity Metrics ===\")\n",
    "for entity, scores in metrics['entity_metrics'].items():\n",
    "    print(f\"{entity}:\")\n",
    "    print(f\"  Precision: {scores['precision']:.4f}\")\n",
    "    print(f\"  Recall:    {scores['recall']:.4f}\")\n",
    "    print(f\"  F1-score:  {scores['f1']:.4f}\")\n",
    "    print(f\"  Support:   {scores['support']}\")\n",
    "print(\"Time inference: \", time.time() - time_start, ' c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0c501820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "квадрат -> product\n",
      "70 -> width\n",
      "100 -> height\n",
      "ст3 -> mark_steal\n",
      "сп -> mark_steal\n",
      "- -> mark_steal\n",
      "2 -> mark_steal\n",
      "6000 -> length\n",
      "гост -> standart_gost\n",
      "2591 -> standart_gost\n",
      "- -> standart_gost\n",
      "88 -> standart_gost\n"
     ]
    }
   ],
   "source": [
    "# Тестирование\n",
    "test = 'квадрат 70 x 100 ст3 сп - 2, 6000 гост 2591 - 88 vvf'\n",
    "test_sentence = tokenizer(test)\n",
    "predicted_tags = predict(model, test_sentence, vocab, tag_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d61262",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
